{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from data_prep import clean_data, dataset\n",
    "from data_prep.utils import shuffle_by_timestep, shuffle_non_padded, masked_mae, compute_iou\n",
    "from models import transformer\n",
    "from train_test import train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\outcome_modeling\\data_prep\\clean_data.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['COURSE'] = df['SUBJECT'] + ' ' + df['COURSE_NUMBER']\n"
     ]
    }
   ],
   "source": [
    "df = clean_data.load_data()\n",
    "df = clean_data.clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mappings used to identify courses from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id_map, id_course_map = clean_data.course_id_maps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_id_map, id_major_map = clean_data.major_id_maps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = course_id_map[\"<PAD>\"]\n",
    "SOS_IDX = course_id_map[\"<SOS>\"]\n",
    "EOS_IDX = course_id_map[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_seqs, student_tensors, semester_tensors, major_tensors, grade_tensors = dataset.course_semester_tensors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5326 students, 360 courses, 48 majors, 18 \"semesters\"\n"
     ]
    }
   ],
   "source": [
    "n_students = len(student_seqs)\n",
    "n_courses = len(course_id_map)\n",
    "n_semesters = df['SEMESTER_RANK'].nunique()\n",
    "n_majors = len(major_id_map) \n",
    "\n",
    "print(f'{n_students} students, {n_courses} courses, {n_majors} majors, {n_semesters} \"semesters\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min courses: 7, Max courses: 58\n"
     ]
    }
   ],
   "source": [
    "student_tensor_sizes = [x.size()[0] for x in student_tensors]\n",
    "print(f'Min courses: {min(student_tensor_sizes)}, Max courses: {max(student_tensor_sizes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataset.course_seq_dataloader(student_tensors, semester_tensors, major_tensors, grade_tensors, n_courses, SOS_IDX, EOS_IDX, PAD_IDX, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['d_model'] = 256\n",
    "config['num_encoder_layers'] = 4\n",
    "config['num_decoder_layers'] = 4\n",
    "config['major_embedding_dim'] = 4\n",
    "config['nhead'] = 4\n",
    "\n",
    "# Given parameters defined in config, function to find all folders in model_fits with the same parameters\n",
    "# Recall that the folder contains a file named config.csv which contains the parameters used to train the model\n",
    "# with the same parameter names, so scan this file to find if this model fits the parameters above.\n",
    "def find_model_fits(config):\n",
    "    model_fits = Path('model_fits')\n",
    "    fits = []\n",
    "    for fit in model_fits.iterdir():\n",
    "        if fit.is_dir():\n",
    "            config_file = fit / 'config.csv'\n",
    "            if config_file.exists():\n",
    "                with open(config_file, 'r') as f:\n",
    "                    folder_config = pd.read_csv(f).to_dict(orient='records')[0]\n",
    "                    if all([(key in folder_config) & (folder_config[key] == config[key]) for key in config]):\n",
    "                        fits.append(fit)\n",
    "    return fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dir = find_model_fits(config)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return config from a fit directory, and load the model\n",
    "def load_model(fit_dir):\n",
    "    config_file = fit_dir / 'config.csv'\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = pd.read_csv(f).to_dict(orient='records')[0]\n",
    "    model = transformer.TransformerModelWithGrades(n_courses=n_courses,\n",
    "                                                    n_majors=n_majors,\n",
    "                                                    max_len=100,\n",
    "                                                    config=config,\n",
    "                                                    PAD_IDX=PAD_IDX).to(device)\n",
    "    \n",
    "    # Load the state dict\n",
    "    state_dict = torch.load(fit_dir / 'model.pyt')\n",
    "\n",
    "    # Remove 'module.' prefix from keys\n",
    "    state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "\n",
    "    # Load the model state dict from model.pyt in directory\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, config = load_model(fit_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_loss_fn = nn.NLLLoss(reduction='mean')\n",
    "gpa_loss_fn = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a single student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student_tensors, semester_tensors, major_tensors, grade_tensors\n",
    "semester_tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_student(min_semesters=4, min_course_per_semester=3, major=None, num_semesters_trim=1):\n",
    "    for i in range(len(student_tensors)):\n",
    "        if max(semester_tensors[i]) >= min_semesters:\n",
    "            if major is None or major_tensors[i] == major:\n",
    "                student_df = pd.DataFrame({'courses': [id_course_map[x.item()] for x in student_tensors[i]],\n",
    "                            'semesters': semester_tensors[i].tolist(),\n",
    "                            'grades': [4.3 * x for x in grade_tensors[i].tolist()]})\n",
    "                min_courses = student_df.groupby('semesters').count().min().values[0]\n",
    "                if min_courses >= min_course_per_semester:\n",
    "                    # Find index where last semester starts\n",
    "                    idx = semester_tensors[i].max(0).indices.item()\n",
    "                    src_student_tensors = student_tensors[i][:idx+1].unsqueeze(0)\n",
    "                    tgt_student_tensors = student_tensors[i][idx:].unsqueeze(0)\n",
    "                    src_semester_tensors = semester_tensors[i][:idx+1].unsqueeze(0)\n",
    "                    tgt_semester_tensors = semester_tensors[i][idx:].unsqueeze(0)\n",
    "                    src_grade_tensors = grade_tensors[i][:idx+1].unsqueeze(0)\n",
    "                    tgt_grade_tensors = grade_tensors[i][idx:].unsqueeze(0)\n",
    "                    major = major_tensors[i]\n",
    "                    return (src_student_tensors, tgt_student_tensors), (src_semester_tensors, tgt_semester_tensors), (src_grade_tensors, tgt_grade_tensors), major, student_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major: Biochemistry\n"
     ]
    }
   ],
   "source": [
    "(src_sample_courses, tgt_sample_courses), (src_sample_semesters, tgt_sample_semesters), (src_sample_grades, tgt_sample_grades), sample_major, student_df = find_student(8)\n",
    "\n",
    "print(f'Major: {id_major_map[sample_major.item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courses</th>\n",
       "      <th>semesters</th>\n",
       "      <th>grades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMM 1317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHIL 1301</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEM 2123</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEM 2323</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;OTHER&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     courses  semesters  grades\n",
       "0  COMM 1317          1     0.0\n",
       "1  PHIL 1301          1     3.0\n",
       "2  CHEM 2123          1     4.0\n",
       "3  CHEM 2323          1     3.0\n",
       "4    <OTHER>          1     3.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred_courses, pred_gpas = train_test.predict(device, model, sample_major, src_sample_courses, src_sample_grades, src_sample_semesters, SOS_IDX, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_courses_names = [id_course_map[x.item()] for x in pred_courses[0] if x not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "pred_grades = [max(x.item() * 4.3, 0) for x in pred_gpas[0]]\n",
    "tgt_courses_names = [id_course_map[x.item()] for x in tgt_sample_courses[0] if x not in [PAD_IDX, SOS_IDX, EOS_IDX]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<OTHER>', 'COSC 3327', 'CAPS 4360'], ['RELS 2323', 'COSC 3337', '<OTHER>'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_courses_names, tgt_courses_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(config, dir, id_course_map=None):\n",
    "    model = transformer.TransformerModelWithGrades(n_courses=n_courses,\n",
    "                                                    n_majors=n_majors,\n",
    "                                                    max_len=100,\n",
    "                                                    config=config,\n",
    "                                                    PAD_IDX=PAD_IDX).to(device)\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=5)\n",
    "\n",
    "    epochs = config['epochs']\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_ious = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}\\n-------------------------------')\n",
    "        train_loss = train_test.train(device, train_dataloader, model, course_loss_fn, gpa_loss_fn, optimizer, scheduler, config, EOS_IDX, PAD_IDX)\n",
    "        test_loss, test_iou = train_test.test(device, test_dataloader, model, course_loss_fn, gpa_loss_fn, config, EOS_IDX, PAD_IDX, id_course_map)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_ious.append(test_iou)\n",
    "    \n",
    "    # Create the directory dir if it does not already exist\n",
    "    Path(f'model_fits/{dir}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save config object as a DataFrame\n",
    "    config_df = pd.DataFrame(config, index=[0])\n",
    "    config_df.to_csv(f'model_fits/{dir}/config.csv', index=False)\n",
    "        \n",
    "    # Save the model state dict\n",
    "    torch.save(model.state_dict(), f'model_fits/{dir}/model.pyt')\n",
    "\n",
    "    # Also save the train and test losses as a single CSV file\n",
    "    losses = pd.DataFrame({'train_loss': train_losses, 'test_loss': test_losses})\n",
    "    losses.to_csv(f'model_fits/{dir}/losses.csv', index=False)\n",
    "\n",
    "    # Save the test IOU\n",
    "    ious = pd.DataFrame({'test_iou': test_ious})\n",
    "    ious.to_csv(f'model_fits/{dir}/ious.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(dir='model_fits'):\n",
    "    config_files = list(Path(dir).rglob('config.csv'))\n",
    "    configs = []\n",
    "    for file in config_files:\n",
    "        config = pd.read_csv(file).iloc[0].to_dict()\n",
    "        configs.append(config)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d_model': 256, 'nhead': 8, 'num_encoder_layers': 4, 'num_decoder_layers': 4, 'dim_feedforward': 512, 'major_embedding_dim': 4, 'course_embedding_dim': 128, 'gpa_embedding_dim': 8, 'dropout': 0.1, 'lr': 0.0001, 'weight_decay': 0.1, 'course_loss_weight': 1, 'epochs': 30}\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.267307 [0.1%] (83.1% course, 16.9% gpa)\n",
      "loss: 2.458854 [22.9%] (95.1% course, 4.9% gpa)\n",
      "loss: 2.309185 [45.7%] (94.2% course, 5.8% gpa)\n",
      "loss: 2.045460 [68.5%] (94.6% course, 5.4% gpa)\n",
      "loss: 2.025536 [91.4%] (94.9% course, 5.1% gpa)\n",
      "LR = 1.00E-04\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'COMM 1317', 'BIOL 1307', 'CHEM 1340', 'BIOL 1307', 'MATH 2312']\n",
      "\tActual: ['BIOL 1107', 'MATH 2413', 'CHEM 1140', 'CHEM 1340', 'BIOL 1307', 'FSEM 1409']\n",
      "Test loss: 1.570552\n",
      "Mean IOU: 29.6%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.945214 [0.1%] (94.4% course, 5.6% gpa)\n",
      "loss: 1.943330 [22.9%] (94.9% course, 5.1% gpa)\n",
      "loss: 1.730824 [45.7%] (93.9% course, 6.1% gpa)\n",
      "loss: 1.798135 [68.5%] (94.9% course, 5.1% gpa)\n",
      "loss: 1.784542 [91.4%] (94.8% course, 5.2% gpa)\n",
      "LR = 9.05E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'COMM 1317', 'MKTG 2301', 'FSTY 1310', 'COSC 1323']\n",
      "\tActual: ['<OTHER>', 'WRIT 1301', 'MATH 2312', 'PSYC 2301', 'COSC 1123', 'COSC 1323']\n",
      "Test loss: 1.355692\n",
      "Mean IOU: 33.1%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.447362 [0.1%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.731041 [22.9%] (95.1% course, 4.9% gpa)\n",
      "loss: 1.513235 [45.7%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.536940 [68.5%] (93.3% course, 6.7% gpa)\n",
      "loss: 1.652150 [91.4%] (94.2% course, 5.8% gpa)\n",
      "LR = 6.55E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'PHIL 2329', 'PHIL 2329']\n",
      "\tActual: ['<OTHER>', 'ARTS 2399', 'ENGL 2324']\n",
      "Test loss: 1.242103\n",
      "Mean IOU: 34.2%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.462126 [0.1%] (93.8% course, 6.2% gpa)\n",
      "loss: 1.500058 [22.9%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.563954 [45.7%] (94.4% course, 5.6% gpa)\n",
      "loss: 1.606655 [68.5%] (93.9% course, 6.1% gpa)\n",
      "loss: 1.471526 [91.4%] (93.8% course, 6.2% gpa)\n",
      "LR = 3.45E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', '<OTHER>', 'PSYC 4359', 'CHEM 2323', 'BIOL 3139']\n",
      "\tActual: ['PHIL 3311', '<OTHER>', 'CHEM 2123', 'CHEM 2323', 'ENGL 2324']\n",
      "Test loss: 1.140661\n",
      "Mean IOU: 35.4%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.432690 [0.1%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.428469 [22.9%] (93.5% course, 6.5% gpa)\n",
      "loss: 1.501508 [45.7%] (94.4% course, 5.6% gpa)\n",
      "loss: 1.438518 [68.5%] (94.0% course, 6.0% gpa)\n",
      "loss: 1.447415 [91.4%] (94.0% course, 6.0% gpa)\n",
      "LR = 9.55E-06\n",
      "Sample\n",
      "\tPredicted: ['BIOL 1107', 'BIOL 1307', 'BIOL 1307', 'MATH 2312', 'FSTY 1310']\n",
      "\tActual: ['BIOL 1107', 'COMM 1317', 'BIOL 1307', 'FRSC 1319', 'CRIJ 1302', 'FSEM 1401']\n",
      "Test loss: 1.315833\n",
      "Mean IOU: 35.9%\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.489667 [0.1%] (93.8% course, 6.2% gpa)\n",
      "loss: 1.536886 [22.9%] (94.0% course, 6.0% gpa)\n",
      "loss: 1.636858 [45.7%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.486648 [68.5%] (94.7% course, 5.3% gpa)\n",
      "loss: 1.544891 [91.4%] (94.3% course, 5.7% gpa)\n",
      "LR = 1.00E-04\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'ACCT 2301', 'COSC 1318', 'MATH 2312']\n",
      "\tActual: ['ECON 2301', 'MGMT 2301', 'CULF 1320', 'CULF 1319', 'SCIE 2320']\n",
      "Test loss: 1.194784\n",
      "Mean IOU: 36.4%\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.497667 [0.1%] (94.0% course, 6.0% gpa)\n",
      "loss: 1.414092 [22.9%] (92.8% course, 7.2% gpa)\n",
      "loss: 1.432378 [45.7%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.392305 [68.5%] (93.8% course, 6.2% gpa)\n",
      "loss: 1.353977 [91.4%] (93.7% course, 6.3% gpa)\n",
      "LR = 9.96E-05\n",
      "Sample\n",
      "\tPredicted: ['WRIT 2302', 'ARTS 1316', 'ARTS 1316', 'VGAM 2318', 'ARTS 1311']\n",
      "\tActual: ['SCIE 2324', 'GLST 1322', 'HIST 1301', 'VISU 1311', 'VGAM 2325', 'BDMM 3334']\n",
      "Test loss: 1.522097\n",
      "Mean IOU: 36.8%\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.269290 [0.1%] (93.8% course, 6.2% gpa)\n",
      "loss: 1.306521 [22.9%] (92.7% course, 7.3% gpa)\n",
      "loss: 1.517170 [45.7%] (94.8% course, 5.2% gpa)\n",
      "loss: 1.364929 [68.5%] (94.7% course, 5.3% gpa)\n",
      "loss: 1.589327 [91.4%] (96.1% course, 3.9% gpa)\n",
      "LR = 9.84E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'SABR 2150', 'PHIL 2329', 'POLS 3328']\n",
      "\tActual: ['<OTHER>', 'SOCI 1301', 'POLS 2332', 'CRIJ 1302', 'CULF 3330']\n",
      "Test loss: 1.672558\n",
      "Mean IOU: 37.6%\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.323462 [0.1%] (93.7% course, 6.3% gpa)\n",
      "loss: 1.324993 [22.9%] (93.9% course, 6.1% gpa)\n",
      "loss: 1.133886 [45.7%] (92.2% course, 7.8% gpa)\n",
      "loss: 1.144205 [68.5%] (93.8% course, 6.2% gpa)\n",
      "loss: 1.470645 [91.4%] (94.1% course, 5.9% gpa)\n",
      "LR = 9.65E-05\n",
      "Sample\n",
      "\tPredicted: ['WRIT 2302', 'ACCT 2301', 'ACCT 2301', 'BUSI 3328']\n",
      "\tActual: ['<OTHER>', 'SPAN 1312', 'MKTG 3335', 'MKTG 4342', 'MKTG 3336']\n",
      "Test loss: 1.270139\n",
      "Mean IOU: 38.4%\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.142156 [0.1%] (93.2% course, 6.8% gpa)\n",
      "loss: 1.341048 [22.9%] (93.6% course, 6.4% gpa)\n",
      "loss: 1.259443 [45.7%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.416745 [68.5%] (94.0% course, 6.0% gpa)\n",
      "loss: 1.117024 [91.4%] (93.0% course, 7.0% gpa)\n",
      "LR = 9.38E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'BUSI 1301', 'MKTG 2301', 'MKTG 2301', 'FSEM 1407']\n",
      "\tActual: ['COMM 1317', 'BUSI 1301', 'ECON 2301', 'POLS 1305', 'FSEM 1401']\n",
      "Test loss: 1.253869\n",
      "Mean IOU: 38.9%\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.286553 [0.1%] (92.8% course, 7.2% gpa)\n",
      "loss: 1.363410 [22.9%] (93.6% course, 6.4% gpa)\n",
      "loss: 1.368720 [45.7%] (94.4% course, 5.6% gpa)\n",
      "loss: 1.231966 [68.5%] (92.5% course, 7.5% gpa)\n",
      "loss: 1.334221 [91.4%] (94.0% course, 6.0% gpa)\n",
      "LR = 9.05E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'BUSI 3333', 'BUSI 3328', 'MGMT 3334', 'MKTG 4342']\n",
      "\tActual: ['PHIL 2329', 'BUSI 3333', 'BUSI 3328', 'MKTG 3343']\n",
      "Test loss: 1.214466\n",
      "Mean IOU: 39.6%\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.266707 [0.1%] (94.6% course, 5.4% gpa)\n",
      "loss: 1.349234 [22.9%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.306435 [45.7%] (95.2% course, 4.8% gpa)\n",
      "loss: 1.298085 [68.5%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.226578 [91.4%] (94.6% course, 5.4% gpa)\n",
      "LR = 8.64E-05\n",
      "Sample\n",
      "\tPredicted: ['KINE 3334', 'KINE 3334', 'BIOL 2401', 'BIOL 2401', 'BIOL 2401']\n",
      "\tActual: ['PHIL 3311', 'KINE 3347', 'KINE 1119', 'KINE 3333', 'BIOL 2401']\n",
      "Test loss: 1.536357\n",
      "Mean IOU: 39.7%\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.237645 [0.1%] (93.3% course, 6.7% gpa)\n",
      "loss: 1.251946 [22.9%] (94.6% course, 5.4% gpa)\n",
      "loss: 1.058611 [45.7%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.136600 [68.5%] (93.5% course, 6.5% gpa)\n",
      "loss: 1.066898 [91.4%] (92.5% course, 7.5% gpa)\n",
      "LR = 8.19E-05\n",
      "Sample\n",
      "\tPredicted: ['PSYC 2310', 'COMM 1317', 'ECON 2302', 'ECON 2302']\n",
      "\tActual: ['ARTS 2399', 'BUSI 1301', 'ACCT 2301', 'COSC 1305', 'BUSI 2305', 'MKTG 3335']\n",
      "Test loss: 1.222979\n",
      "Mean IOU: 39.6%\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.196495 [0.1%] (92.9% course, 7.1% gpa)\n",
      "loss: 1.196777 [22.9%] (93.9% course, 6.1% gpa)\n",
      "loss: 1.108427 [45.7%] (93.6% course, 6.4% gpa)\n",
      "loss: 1.335098 [68.5%] (94.7% course, 5.3% gpa)\n",
      "loss: 1.230850 [91.4%] (92.9% course, 7.1% gpa)\n",
      "LR = 7.68E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'CHEM 3325', 'CHEM 4157', 'PHYS 2321', 'CHEM 4157']\n",
      "\tActual: ['CHEM 3125', 'CHEM 3325', 'PHYS 2126', 'PHYS 2321', 'SCIE 4345', 'MATH 2321']\n",
      "Test loss: 1.161063\n",
      "Mean IOU: 40.6%\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.368812 [0.1%] (93.7% course, 6.3% gpa)\n",
      "loss: 1.306595 [22.9%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.266930 [45.7%] (94.9% course, 5.1% gpa)\n",
      "loss: 1.244286 [68.5%] (94.5% course, 5.5% gpa)\n",
      "loss: 1.185090 [91.4%] (93.4% course, 6.6% gpa)\n",
      "LR = 7.13E-05\n",
      "Sample\n",
      "\tPredicted: ['BIOL 2334', 'SABR 2150']\n",
      "\tActual: ['<OTHER>', 'SABR 2150']\n",
      "Test loss: 1.315980\n",
      "Mean IOU: 40.5%\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.055034 [0.1%] (92.6% course, 7.4% gpa)\n",
      "loss: 1.124784 [22.9%] (94.3% course, 5.7% gpa)\n",
      "loss: 1.140950 [45.7%] (95.1% course, 4.9% gpa)\n",
      "loss: 1.198506 [68.5%] (92.8% course, 7.2% gpa)\n",
      "loss: 1.085335 [91.4%] (93.3% course, 6.7% gpa)\n",
      "LR = 6.55E-05\n",
      "Sample\n",
      "\tPredicted: ['BIOL 1107', 'BIOL 1307', 'BIOL 1307', 'CHEM 1340', 'BIOL 1307', 'PSYC 2301', 'FSEM 1405']\n",
      "\tActual: ['BIOL 1107', 'WRIT 1301', 'CHEM 1140', 'CHEM 1340', 'BIOL 1307', 'SCIE 1100', 'FSEM 1405']\n",
      "Test loss: 0.962026\n",
      "Mean IOU: 41.0%\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.005809 [0.1%] (92.7% course, 7.3% gpa)\n",
      "loss: 1.078062 [22.9%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.151721 [45.7%] (93.5% course, 6.5% gpa)\n",
      "loss: 1.005234 [68.5%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.093381 [91.4%] (94.4% course, 5.6% gpa)\n",
      "LR = 5.94E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'BUSI 1301', 'MKTG 2301', 'MATH 2312', 'MATH 2312']\n",
      "\tActual: ['COMM 1317', 'WRIT 1301', 'MKTG 2301', 'SPAN 1312', 'FSEM 1405']\n",
      "Test loss: 1.815084\n",
      "Mean IOU: 40.4%\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.241981 [0.1%] (95.2% course, 4.8% gpa)\n",
      "loss: 1.131999 [22.9%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.129057 [45.7%] (94.8% course, 5.2% gpa)\n",
      "loss: 1.043354 [68.5%] (92.2% course, 7.8% gpa)\n",
      "loss: 1.198263 [91.4%] (93.8% course, 6.2% gpa)\n",
      "LR = 5.31E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', '<OTHER>', 'WRIT 2320']\n",
      "\tActual: ['WRIT 2302', '<OTHER>']\n",
      "Test loss: 0.840716\n",
      "Mean IOU: 41.1%\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.146487 [0.1%] (93.6% course, 6.4% gpa)\n",
      "loss: 1.082834 [22.9%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.046494 [45.7%] (93.6% course, 6.4% gpa)\n",
      "loss: 1.017475 [68.5%] (92.4% course, 7.6% gpa)\n",
      "loss: 1.229796 [91.4%] (94.1% course, 5.9% gpa)\n",
      "LR = 4.69E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'HONS 2160', 'COMM 3344', 'COMM 3344', 'COMM 2312']\n",
      "\tActual: ['<OTHER>', 'HONS 2160', 'SCIE 2324', 'COMM 3344']\n",
      "Test loss: 1.054083\n",
      "Mean IOU: 40.9%\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.048987 [0.1%] (92.9% course, 7.1% gpa)\n",
      "loss: 1.014691 [22.9%] (92.4% course, 7.6% gpa)\n",
      "loss: 1.131163 [45.7%] (92.1% course, 7.9% gpa)\n",
      "loss: 1.011586 [68.5%] (94.1% course, 5.9% gpa)\n",
      "loss: 1.013406 [91.4%] (93.2% course, 6.8% gpa)\n",
      "LR = 4.06E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'WRIT 1301', 'MATH 309']\n",
      "\tActual: ['<OTHER>', 'SPAN 1311', 'RELS 1316', 'ENGL 2324', 'HIST 1302']\n",
      "Test loss: 1.165959\n",
      "Mean IOU: 41.1%\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.101502 [0.1%] (92.1% course, 7.9% gpa)\n",
      "loss: 1.149495 [22.9%] (92.9% course, 7.1% gpa)\n",
      "loss: 1.121898 [45.7%] (93.3% course, 6.7% gpa)\n",
      "loss: 1.211394 [68.5%] (93.7% course, 6.3% gpa)\n",
      "loss: 1.077613 [91.4%] (94.2% course, 5.8% gpa)\n",
      "LR = 3.45E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'BUSI 3328', 'MKTG 4342', 'BIOL 1322']\n",
      "\tActual: ['FINC 3330', 'MKTG 3343', 'WRIT 2311', 'MKTG 3336']\n",
      "Test loss: 1.156679\n",
      "Mean IOU: 41.3%\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.034778 [0.1%] (93.4% course, 6.6% gpa)\n",
      "loss: 0.893221 [22.9%] (93.7% course, 6.3% gpa)\n",
      "loss: 1.091534 [45.7%] (93.2% course, 6.8% gpa)\n",
      "loss: 1.286706 [68.5%] (95.1% course, 4.9% gpa)\n",
      "loss: 0.994870 [91.4%] (92.5% course, 7.5% gpa)\n",
      "LR = 2.87E-05\n",
      "Sample\n",
      "\tPredicted: ['PSYC 2321', 'WRIT 2302', 'PSYC 2316', 'RELS 1315']\n",
      "\tActual: ['PSYC 2321', 'COMM 1317', 'PSYC 2316', 'FREN 1311', 'EDUC 1330']\n",
      "Test loss: 1.059312\n",
      "Mean IOU: 41.3%\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.241182 [0.1%] (93.2% course, 6.8% gpa)\n",
      "loss: 1.041570 [22.9%] (91.9% course, 8.1% gpa)\n",
      "loss: 1.053355 [45.7%] (92.3% course, 7.7% gpa)\n",
      "loss: 0.944492 [68.5%] (92.4% course, 7.6% gpa)\n",
      "loss: 1.131252 [91.4%] (93.7% course, 6.3% gpa)\n",
      "LR = 2.32E-05\n",
      "Sample\n",
      "\tPredicted: ['CHEM 2120', 'CHEM 2120', 'CHEM 2120', 'CHEM 2320', 'MATH 3320', 'BIOL 2401']\n",
      "\tActual: ['RELS 1304', 'SPAN 1312', 'CHEM 2120', 'CHEM 2320', 'MATH 3320', 'CULF 3330']\n",
      "Test loss: 1.180178\n",
      "Mean IOU: 41.5%\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.962844 [0.1%] (91.9% course, 8.1% gpa)\n",
      "loss: 0.933313 [22.9%] (92.1% course, 7.9% gpa)\n",
      "loss: 1.083335 [45.7%] (93.2% course, 6.8% gpa)\n",
      "loss: 1.103142 [68.5%] (93.7% course, 6.3% gpa)\n",
      "loss: 0.991462 [91.4%] (91.4% course, 8.6% gpa)\n",
      "LR = 1.81E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'CHEM 3343']\n",
      "\tActual: ['<OTHER>', 'CHEM 4157', 'SCIE 4345']\n",
      "Test loss: 1.594732\n",
      "Mean IOU: 41.1%\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.003443 [0.1%] (92.2% course, 7.8% gpa)\n",
      "loss: 1.100735 [22.9%] (92.6% course, 7.4% gpa)\n",
      "loss: 0.965410 [45.7%] (92.4% course, 7.6% gpa)\n",
      "loss: 1.147980 [68.5%] (93.4% course, 6.6% gpa)\n",
      "loss: 1.105310 [91.4%] (91.9% course, 8.1% gpa)\n",
      "LR = 1.36E-05\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', '<OTHER>', 'BIOL 1322', 'PSYC 4345', 'PSYC 2349']\n",
      "\tActual: ['PSYC 2326', 'SPAN 1311', 'COMM 3344', 'PSYC 4345', 'PSYC 4349']\n",
      "Test loss: 1.222691\n",
      "Mean IOU: 41.0%\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.119442 [0.1%] (93.9% course, 6.1% gpa)\n",
      "loss: 1.094548 [22.9%] (93.4% course, 6.6% gpa)\n",
      "loss: 0.961345 [45.7%] (92.9% course, 7.1% gpa)\n",
      "loss: 0.981252 [68.5%] (94.4% course, 5.6% gpa)\n",
      "loss: 1.082683 [91.4%] (94.0% course, 6.0% gpa)\n",
      "LR = 9.55E-06\n",
      "Sample\n",
      "\tPredicted: ['COMM 1317', 'MKTG 2301', 'CULF 1319', 'COSC 2328', 'MATH 2315']\n",
      "\tActual: ['ECON 2301', 'ACCT 2301', 'CULF 1319', 'COSC 2329', 'MATH 2315']\n",
      "Test loss: 1.018428\n",
      "Mean IOU: 41.5%\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.895195 [0.1%] (92.4% course, 7.6% gpa)\n",
      "loss: 0.869610 [22.9%] (91.8% course, 8.2% gpa)\n",
      "loss: 1.066630 [45.7%] (93.1% course, 6.9% gpa)\n",
      "loss: 1.049515 [68.5%] (92.9% course, 7.1% gpa)\n",
      "loss: 1.134276 [91.4%] (93.6% course, 6.4% gpa)\n",
      "LR = 6.18E-06\n",
      "Sample\n",
      "\tPredicted: ['BIOL 2334', '<OTHER>', 'CHEM 2320', 'MATH 3320', 'ENGL 2324']\n",
      "\tActual: ['BIOL 2334', 'CHEM 2120', 'CHEM 2320', 'MATH 3320', 'PSYC 2301']\n",
      "Test loss: 0.813203\n",
      "Mean IOU: 41.5%\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.990369 [0.1%] (92.7% course, 7.3% gpa)\n",
      "loss: 1.022711 [22.9%] (91.4% course, 8.6% gpa)\n",
      "loss: 1.072437 [45.7%] (94.9% course, 5.1% gpa)\n",
      "loss: 0.976541 [68.5%] (92.4% course, 7.6% gpa)\n",
      "loss: 0.961796 [91.4%] (92.4% course, 7.6% gpa)\n",
      "LR = 3.51E-06\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'PHIL 2329', 'MATH 3320', 'SPED 2324']\n",
      "\tActual: ['<OTHER>', 'EDUC 3336', 'MATH 3320']\n",
      "Test loss: 0.994741\n",
      "Mean IOU: 41.4%\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.878387 [0.1%] (91.5% course, 8.5% gpa)\n",
      "loss: 1.017061 [22.9%] (93.6% course, 6.4% gpa)\n",
      "loss: 0.960721 [45.7%] (94.2% course, 5.8% gpa)\n",
      "loss: 0.982247 [68.5%] (93.0% course, 7.0% gpa)\n",
      "loss: 0.772462 [91.4%] (91.4% course, 8.6% gpa)\n",
      "LR = 1.57E-06\n",
      "Sample\n",
      "\tPredicted: ['<OTHER>', 'MUSI 2143']\n",
      "\tActual: ['<OTHER>', 'BUSI 3328']\n",
      "Test loss: 1.241570\n",
      "Mean IOU: 41.3%\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.018404 [0.1%] (93.4% course, 6.6% gpa)\n",
      "loss: 0.890752 [22.9%] (93.3% course, 6.7% gpa)\n",
      "loss: 0.981712 [45.7%] (94.2% course, 5.8% gpa)\n",
      "loss: 1.090949 [68.5%] (92.7% course, 7.3% gpa)\n",
      "loss: 1.004999 [91.4%] (93.4% course, 6.6% gpa)\n",
      "LR = 3.94E-07\n",
      "Sample\n",
      "\tPredicted: ['ECON 2301', 'ECON 2301', 'MKTG 2301', 'MATH 1324', 'BIOL 1310']\n",
      "\tActual: ['<OTHER>', 'ECON 2301', 'MKTG 2301', 'MATH 1324', 'PHYS 1310']\n",
      "Test loss: 0.666362\n",
      "Mean IOU: 41.7%\n",
      "\n",
      "{'d_model': 256, 'nhead': 4, 'num_encoder_layers': 4, 'num_decoder_layers': 4, 'dim_feedforward': 512, 'major_embedding_dim': 4, 'course_embedding_dim': 128, 'gpa_embedding_dim': 16, 'dropout': 0.1, 'lr': 0.0001, 'weight_decay': 0.1, 'course_loss_weight': 1, 'epochs': 30}\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.278923 [0.1%] (95.4% course, 4.6% gpa)\n",
      "loss: 2.373814 [22.9%] (95.9% course, 4.1% gpa)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_enc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_encoder_layers\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_dec_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_decoder_layers\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_maj_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmajor_embedding_dim\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_cou_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcourse_embedding_dim\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_nhead_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnhead\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_dmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43md_model\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_course_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(config, dir, id_course_map)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcourse_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpa_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEOS_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPAD_IDX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     test_loss, test_iou \u001b[38;5;241m=\u001b[39m train_test\u001b[38;5;241m.\u001b[39mtest(device, test_dataloader, model, course_loss_fn, gpa_loss_fn, config, EOS_IDX, PAD_IDX, id_course_map)\n\u001b[0;32m     24\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\outcome_modeling\\train_test\\train_test.py:53\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(device, dataloader, model, course_loss_fn, gpa_loss_fn, optimizer, scheduler, config, EOS_IDX, PAD_IDX)\u001b[0m\n\u001b[0;32m     50\u001b[0m src_gpas \u001b[38;5;241m=\u001b[39m src_gpas\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m output_courses_lsm, output_gpas, output_courses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43msrc_courses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_courses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtgt_courses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_courses_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43msrc_gpas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_gpas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtgt_gpas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_grades_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43msrc_courses_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_courses_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtgt_courses_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_pos_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Mask the padding values when computing loss\u001b[39;00m\n\u001b[0;32m     63\u001b[0m course_loss \u001b[38;5;241m=\u001b[39m course_loss_fn(output_courses_lsm, tgt_out)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\Projects\\NSCI-retention-modeling\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:100\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m     98\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[1;32m--> 100\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in [1e-4, 1e-3]:\n",
    "    for num_encoder_layers in [4]:\n",
    "        for num_decoder_layers in [4, 6]:\n",
    "            for major_embedding_dim in [2**2, 2**3, 2**4, 2**5]:\n",
    "                for course_embedding_dim in [2**7, 2**8]:\n",
    "                    for gpa_embedding_dim in [2**3, 2**4]:\n",
    "                        for nhead in [4, 8]:\n",
    "                            for d_model in [2**8]:\n",
    "                                config['lr'] = lr\n",
    "                                config['num_encoder_layers'] = num_encoder_layers\n",
    "                                config['num_decoder_layers'] = num_decoder_layers\n",
    "                                config['major_embedding_dim'] = major_embedding_dim\n",
    "                                config['course_embedding_dim'] = course_embedding_dim\n",
    "                                config['gpa_embedding_dim'] = gpa_embedding_dim\n",
    "                                config['nhead'] = nhead\n",
    "                                config['d_model'] = d_model\n",
    "                                config['epochs'] = 30\n",
    "\n",
    "                                config['dim_feedforward'] = 2 * config['d_model']\n",
    "\n",
    "                                all_configs = read_configs()\n",
    "                                if config in all_configs:\n",
    "                                    continue\n",
    "\n",
    "                                print(config)\n",
    "\n",
    "                                fit(config, f'lr_{lr}_enc_{num_encoder_layers}_dec_{num_decoder_layers}_maj_{major_embedding_dim}_cou_{course_embedding_dim}_nhead_{nhead}_dmodel_{d_model}', id_course_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
